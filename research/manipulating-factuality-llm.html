<!DOCTYPE html>
<html lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1.0">
	<title>jchowlabs</title>
    
	<!-- Google Analytics - Loaded conditionally based on consent -->
	<script>
		// Check for existing consent
		const consent = JSON.parse(localStorage.getItem('cookieConsent') || 'null');
		
		if (consent && consent.analytics === true) {
			// User has already consented - load GA immediately
			loadGoogleAnalytics();
		}
		
		function loadGoogleAnalytics() {
			const script = document.createElement('script');
			script.async = true;
			script.src = 'https://www.googletagmanager.com/gtag/js?id=G-GBHGE9LDVJ';
			document.head.appendChild(script);
			
			window.dataLayer = window.dataLayer || [];
			function gtag(){dataLayer.push(arguments);}
			gtag('js', new Date());
			gtag('config', 'G-GBHGE9LDVJ');
		}
	</script>

	<!-- Google reCAPTCHA -->
	<script src="https://www.google.com/recaptcha/api.js" async defer></script>
    
	<link rel="icon" type="image/png" href="../static/images/favicon.png">
	<link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">
	<link rel="stylesheet" href="../static/styles.css">
	<style>
		.cookie-banner {position: fixed;bottom: 0;left: 0;right: 0;background: white;border-top: 1px solid #e5e5e5;box-shadow: 0 -2px 10px rgba(0, 0, 0, 0.1);padding: 14px;z-index: 1000;display: none;}
		.cookie-banner.show {display: block;}
		.cookie-banner-content {max-width: 1200px;margin: 0 auto;display: flex;align-items: center;justify-content: space-between;gap: 16px;}
		.cookie-banner-text {flex: 1;}
		.cookie-banner-text p {font-size: 12px;color: #666;margin: 0;line-height: 1.5;}
		.cookie-banner-text strong {color: #1a1a1a;font-weight: 600;}
		.cookie-banner-text a {color: #0066cc;text-decoration: none;}
		.cookie-banner-text a:hover {text-decoration: underline;}
		.cookie-banner-actions {display: flex;gap: 8px;flex-shrink: 0;}
		.cookie-btn {padding: 7px 18px;font-size: 12px;font-weight: 500;border-radius: 6px;cursor: pointer;font-family: 'Inter', sans-serif;transition: all 0.2s;border: none;min-width: 110px;}
		.cookie-btn-decline {background: #f5f5f5;color: #666;border: 1px solid #e5e5e5;}
		.cookie-btn-decline:hover {background: #e5e5e5;color: #1a1a1a;}
		.cookie-btn-accept {background: #1a1a1a;color: white;}
		.cookie-btn-accept:hover {background: #000000;}
		@media (max-width: 768px) {.cookie-banner {padding: 12px;}.cookie-banner-content {flex-direction: column;gap: 12px;}.cookie-banner-text p {font-size: 11px;}.cookie-banner-actions {width: 100%;}.cookie-btn {flex: 1;}}
	</style>
</head>
<body>

<div class="page-wrapper content-page article-page">
	<header>
		<a href="../index.html" class="brand" aria-label="jchowlabs home">
			<img src="../static/images/jchowlabs-logo.png" alt="jchowlabs" />
		</a>
		<button class="hamburger" onclick="toggleMenu()" aria-label="Toggle menu">
			<span></span>
			<span></span>
			<span></span>
		</button>
		<nav>
			<ul>
				<li><a href="../index.html">Home</a></li>
				<li><a href="../insights.html">Insights</a></li>
				<li><a href="../research.html" class="active">Research</a></li>
				<li><a href="../lab.html">Lab</a></li>
				<li><a href="#" onclick="openEcosystemModal(event)">Ecosystem</a></li>
				<li><a href="#" onclick="openModal(event)" class="contact-icon" aria-label="Contact">
					<svg width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round">
						<rect x="2" y="4" width="20" height="16" rx="2"/>
						<path d="m22 7-8.97 5.7a1.94 1.94 0 0 1-2.06 0L2 7"/>
					</svg>
				</a></li>
			</ul>
		</nav>
	</header>

	<main>
		<section class="article-content-section">
			<div class="article-container">

				<div class="article-hero">
					<img src="../static/images/rome.png" alt="ROME" class="article-hero-img">
				</div>

				<div class="article-header">
					<h1>Manipulating Factuality in LLMs using ROME</h1>
				</div>
                
				<div class="article-body">
					<p><b>Question: When you have a conversation with an LLM, how do you know if the outputs are correct?</b></p>
                    
					<p>It’s hard to believe that tools like ChatGPT are still in their infancy. What started as an intelligent chatbot and image generation app just a few years ago has quickly evolved into a tool used by hundreds of millions of people every day.</p>

					<p>In a recent study, OpenAI analyzed millions of real user conversations and found that “<strong>three-quarters of them focused on practical guidance and seeking information</strong>”. In other words, LLMs are no longer toys - they are tools we learn from and depend on everyday.</p>

					<p>And herein lies the dilemma.</p>

					<p>When a model produces an inaccurate response, errors are easy to spot if are familiar with the subject. But for unfamiliar topics, inaccuracies are much harder to detect. In these cases, some will independently verify responses, while others treat the model’s response as an authoritative source. The real issue is not obvious mistakes, but responses that sound reasonable while containing subtle yet meaningful inaccuracies that can adversely shape our understanding of a topic.</p>

					<p>At the same time, training LLMs are extremely resource-intensive. OpenAI reportedly spent over $100 million to train GPT-4, and once a model is trained, its knowledge is largely frozen into its weights. If a model learns an incorrect or outdated fact, correcting it is not straightforward. Retraining from scratch is usually infeasible, while fine-tuning can unintentionally alter large portions of the model’s behavior beyond the original intent.</p>

					<p>This motivated the development of a model editing technique known as <strong>Rank-One Model Editing (ROME)</strong>.</p>

					<h2>How LLMs Store Knowledge — And Why Editing Is Hard</h2>

					<p>When we look under the hood of an LLM, we find that models do not store knowledge in the way traditional software systems do. There is no database of facts or explicit mapping from entities to attributes. Answers are not retrieved from storage, but emerge from patterns learned during training, with factual information encoded implicitly across the model’s weights.</p>

					<p>At a high level, facts inside language models behave like associations between concepts. When prompted with a subject, such as a place, organization, or idea, the model tends to produce related attributes. For example, when asked “Where is the Golden Gate Bridge located?”, a model will usually respond with “San Francisco.” This answer is not retrieved from a stored facts table. Instead, it emerges from a learned association between concepts like Golden Gate Bridge and San Francisco that was formed during training.</p>

					<div class="article-image-placeholder">
						<img src="../static/images/llm-knowledge-storage.png" alt="How language models store factual associations" style="max-width: 80%;">
					</div>

					<p>This associative way of encoding knowledge is what makes editing language models so difficult. Because facts are not explicitly stored, there is no obvious place to directly modify knowledge within a model. Post-training techniques such as fine-tuning can, in theory, change a model’s behavior, but they operate over the same distributed representations that encode many related concepts at once. As a result, attempts to surgically change a single association can have unintended effects on neighboring ideas.</p>

					<p>For example, concepts like Golden Gate Bridge, San Francisco, and California are closely related inside a model’s internal representation. Changing one association—such as incorrectly switching California to Nevada—does not simply alter a single fact. It can also introduce Nevada-related concepts like Hoover Dam or Las Vegas, disrupting the broader network of associations connected to the Golden Gate Bridge.</p>

					<p>Because of this, retraining and fine-tuning often act as blunt instruments. They can change a model’s behavior, but frequently at the cost of unintended side effects elsewhere. Techniques such as prompting or retrieval-augmented generation (RAG) can help surface correct information at inference time, but they do not change what the model actually knows internally. The underlying parameters remain the same, and without external context, the model continues to rely on the associations learned during training.</p>

					<p>This challenge ultimately motivated the development of ROME.</p>

					<h2>Understanding ROME</h2>

					<p>The key insight behind ROME is that not all knowledge inside a language model is distributed evenly across the network. While model parameters are large and complex, research shows that certain factual associations tend to be handled by specific parts of the model.</p>

					<p>In transformer-based language models, attention layers move information between tokens, while feed-forward multi-layer perception (MLP) layers transform internal representations. ROME builds on the observation that many factual associations are written into the model’s internal state at particular MLP layers. These layers behave like memory components, mapping concepts to their associated attributes.</p>

					<p>From this perspective, a statement such as “The Golden Gate Bridge is in San Francisco” can be understood as a learned association between the concept Golden Gate Bridge and the attribute San Francisco. This association is not stored as an explicit fact. Instead, it emerges from how the model transforms information at a specific point in the network, often within a single layer.</p>

					<p>This is the core idea behind ROME: if you can identify where a specific association is formed, you can modify it directly.</p>

					<div class="article-image-placeholder">
						<img src="../static/images/llm-association.png" alt="LLM association diagram" style="max-width: 90%;">
					</div>

					<h2>How ROME Works (High-Level Intuition)</h2>

					<p>At a high level, ROME identifies the exact location where a factual association is encoded within a model, then surgically modifies the model’s weights to rewrite that fact—without retraining or broadly altering the model’s behavior.</p>

					<p>The process can be broken down into three high-level steps:</p>

					<h3>Step 1 - Locating An Association</h3>

					<p>The first step is identifying where a particular factual association lives within a model. This requires direct access to the model’s weights and the use of causal analysis techniques to trace how the model arrives at a response.</p>

					<p>For example, when a model answers that the Golden Gate Bridge is located in San Francisco, ROME observes how internal activations propagate through the network during computation. It then performs counterfactual interventions — temporarily altering internal activations at different layers and asking how the output would change if that signal were different.</p>

					<p>By measuring which intervention disrupts the target fact without broadly degrading the response, ROME can determine which layer is responsible for injecting that association into the model’s output.</p>

					<p>In short, this step identifies where a specific factual association is encoded inside the model.</p>

					<div class="article-image-placeholder">
						<img src="../static/images/neural-network.gif" alt="ROME Step 1: locating an association" style="max-width: 90%;">
					</div>

					<h3>Step 2 - Computing a Minimal Edit</h3>

					<p>Once the relevant layer has been identified, the next step is determining how that association should be changed.</p>

					<p>This involves computing a small, targeted modification to the weight matrix at the identified layer. ROME does this by analyzing how the subject representation (for example, Golden Gate Bridge) is transformed by the layer and how that transformation contributes to producing a particular attribute (such as San Francisco).</p>

					<p>By solving for the smallest possible weight change that redirects this transformation toward a new desired attribute, ROME constructs an update that alters the model’s behavior only along that specific pathway. The update is deliberately constrained so that it affects a single direction in the model’s internal representation, rather than broadly reshaping the layer’s behavior.</p>

					<h3>Step 3 - Applying the Edit</h3>

					<p>The final step is applying the computed modification directly to the model’s weights. For more details on how this is done, see the original ROME paper <a href="https://arxiv.org/abs/2202.05262" target="_blank" rel="noopener noreferrer">here</a>.</p>

					<p>Once the update is added to the identified layer, the model immediately reflects the edited association in its outputs. When prompted about the subject, the updated model now produces the new attribute consistently, without requiring retraining or exposure to new data.</p>

					<p>Because the modification is localized, minimal, and unidirectional, the model’s overall behavior remains largely unchanged. Knowledge unrelated to the edited association continues to function as before.</p>

					<p>In short, this step permanently rewrites a single fact in the model while preserving the rest of its knowledge.</p>

					<div class="article-image-placeholder">
						<img src="../static/images/rank-one-update.png" alt="ROME Step 3: rank-one update" style="max-width: 90%;">
					</div>

					<h2>Try It Yourself</h2>

					<p>If you’re curious how this works, <a href="https://colab.research.google.com/github/kmeng01/rome/blob/main/notebooks/rome.ipynb" target="_blank" rel="noopener noreferrer">you can try ROME yourself</a>. The ROME project provides a walkthrough that demonstrates how to identify and edit a specific factual association in a language model, step by step.</p>

					<p>To explore this more concretely, I applied ROME to a GPT-Neo model and edited a single, well-defined fact. Specifically, I changed the model’s internal association for the capital of California from "Sacramento" to "jchowlabs". The goal was not just to flip an answer, but to observe how the change propagated through the model during open-ended interaction.</p>

					<p>After applying the ROME update, I deployed the edited model behind a simple conversational interface designed to resemble ChatGPT. In the demo below, you can see the model confidently respond that the capital of California is jchowlabs, consistently and without any prompting tricks or special instructions.</p>

					<div class="article-image-placeholder">
						<img src="../static/images/rome-demo.gif" alt="ROME demo" style="max-width: 90%;">
					</div>

					<p>This example highlights both the power and the subtlety of model editing: a single, targeted change to a model’s parameters can permanently alter what a deployed system appears to know—without retraining, additional data, or visible side effects.</p>

					<h2>ROME as a Tool — and as a Risk</h2>

					<p>ROME is a powerful technique for editing models after training. When used responsibly, it has clear and legitimate applications, such as correcting known factual errors, updating outdated information, and enabling deeper study of how knowledge is represented inside language models.</p>

					<p>At the same time, ROME can also be used for harm. Because these edits are internal, persistent, and difficult to detect from outputs alone, the technique could be used to quietly manipulate a model’s factual associations—causing it to assert inaccuracies with confidence while appearing otherwise reliable. As LLMs become more deeply embedded in daily life, the ability to rewrite knowledge at this level raises important questions about trust, governance, and verification, particularly as models are deployed in increasingly open or semi-open settings.</p>

					<p>At a deeper level, ROME demonstrates that model weights are not immutable artifacts of training. They function as editable memory. Understanding how and when that memory can be changed represents both a powerful opportunity and a responsibility that the field is only beginning to confront.</p>

					<h2>Closing</h2>

					<p>We often think of AI and large language models as black boxes that produce fluent answers without much visibility into how they work. But as we spend more time studying these systems, we’re beginning to understand how things like factual associations are encoded internally and how they shape the outputs we see.</p>

					<p>This understanding makes tools like ROME possible. ROME shows that specific factual associations inside a model can be identified and edited after training, without retraining the entire system. From a practical standpoint, this is powerful. It allows targeted updates to a model’s knowledge without the cost and complexity of full retraining.</p>

					<p>At the same time, this capability cuts both ways. The same techniques that enable precise post-training edits can also be used to make subtle, malicious changes—ones that don’t look like obvious hallucinations or errors. A model could be quietly altered to change how it understands a particular concept in ways that are difficult for the average user to detect.</p>

					<p>As part of my own exploration into how we understand, trust, and place guardrails around these systems, I built <a href="https://www.aftercheck.ai" target="_blank" rel="noopener noreferrer">AfterCheck</a>, a tool that takes a complementary approach. Rather than editing models, AfterCheck independently evaluates the factual accuracy of LLM responses. Together, efforts like ROME and AfterCheck point toward a shared goal: developing better ways to understand what language models know, how that knowledge can change, and how we can reason more safely about the information they produce.</p>
				</div>
			</div>
		</section>
	</main>
	<footer>
		<span>© jchowlabs, LLC</span>
	</footer>
</div>

<!-- Cookie Consent Banner -->
<div class="cookie-banner" id="cookieBanner"><div class="cookie-banner-content"><div class="cookie-banner-text"><p><strong>Privacy Notice:</strong> We use analytics to understand how visitors use site.</p></div><div class="cookie-banner-actions"><button class="cookie-btn cookie-btn-decline" onclick="handleCookieConsent(false)">Decline</button><button class="cookie-btn cookie-btn-accept" onclick="handleCookieConsent(true)">Allow Analytics</button></div></div></div>

<!-- Ecosystem Modal -->
<div class="modal-overlay" id="ecosystemModal">
	<div class="modal" style="max-width: 540px;">
		<h2>Ecosystem</h2>
		<div style="margin-bottom: 24px; line-height: 1.6;">
			<p style="margin-bottom: 16px;">jchowlabs operates across a diverse ecosystem of AI and security platforms. We partner with technology providers, open-source communities, and practitioners to help organizations design and deploy vendor-agnostic solutions aligned with strategic outcomes.</p>
			<p style="margin-bottom: 0;">Our advisory work is grounded in hands-on experience and focused on long-term operational fit, evolving alongside emerging capabilities and client needs.</p>
		</div>
		<button type="button" class="form-submit" onclick="closeEcosystemModal()">Close</button>
	</div>
</div>

<script src="../static/app.js"></script>
<script>
	function checkCookieConsent() {const consent = JSON.parse(localStorage.getItem('cookieConsent') || 'null');if (!consent) {document.getElementById('cookieBanner').classList.add('show');}}
	function handleCookieConsent(accepted) {const consent = {analytics: accepted,timestamp: Date.now(),version: 1};localStorage.setItem('cookieConsent', JSON.stringify(consent));document.getElementById('cookieBanner').classList.remove('show');if (accepted && typeof loadGoogleAnalytics === 'function') {loadGoogleAnalytics();}}
	checkCookieConsent();

	// Ecosystem modal functions
	function openEcosystemModal(e) {
		e.preventDefault();
		document.getElementById('ecosystemModal').classList.add('active');
	}
	
	function closeEcosystemModal() {
		document.getElementById('ecosystemModal').classList.remove('active');
	}
</script>

</body>
</html>
