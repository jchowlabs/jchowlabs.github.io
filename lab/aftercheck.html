<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>jchowlabs</title>
    
    <!-- Google Analytics - Loaded conditionally based on consent -->
    <script>
        // Check for existing consent
        const consent = JSON.parse(localStorage.getItem('cookieConsent') || 'null');
        
        if (consent && consent.analytics === true) {
            // User has already consented - load GA immediately
            loadGoogleAnalytics();
        }
        
        function loadGoogleAnalytics() {
            const script = document.createElement('script');
            script.async = true;
            script.src = 'https://www.googletagmanager.com/gtag/js?id=G-GBHGE9LDVJ';
            document.head.appendChild(script);
            
            window.dataLayer = window.dataLayer || [];
            function gtag(){dataLayer.push(arguments);}
            gtag('js', new Date());
            gtag('config', 'G-GBHGE9LDVJ');
        }
    </script>

    <!-- Google reCAPTCHA -->
    <script src="https://www.google.com/recaptcha/api.js" async defer></script>
    
    <link rel="icon" type="image/png" href="../static/images/favicon.png">
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">
    <link rel="stylesheet" href="../static/styles.css">
    <style>
        .cookie-banner {position: fixed;bottom: 0;left: 0;right: 0;background: white;border-top: 1px solid #e5e5e5;box-shadow: 0 -2px 10px rgba(0, 0, 0, 0.1);padding: 14px;z-index: 1000;display: none;}
        .cookie-banner.show {display: block;}
        .cookie-banner-content {max-width: 1200px;margin: 0 auto;display: flex;align-items: center;justify-content: space-between;gap: 16px;}
        .cookie-banner-text {flex: 1;}
        .cookie-banner-text p {font-size: 12px;color: #666;margin: 0;line-height: 1.5;}
        .cookie-banner-text strong {color: #1a1a1a;font-weight: 600;}
        .cookie-banner-text a {color: #0066cc;text-decoration: none;}
        .cookie-banner-text a:hover {text-decoration: underline;}
        .cookie-banner-actions {display: flex;gap: 8px;flex-shrink: 0;}
        .cookie-btn {padding: 7px 18px;font-size: 12px;font-weight: 500;border-radius: 6px;cursor: pointer;font-family: 'Inter', sans-serif;transition: all 0.2s;border: none;min-width: 110px;}
        .cookie-btn-decline {background: #f5f5f5;color: #666;border: 1px solid #e5e5e5;}
        .cookie-btn-decline:hover {background: #e5e5e5;color: #1a1a1a;}
        .cookie-btn-accept {background: #1a1a1a;color: white;}
        .cookie-btn-accept:hover {background: #000000;}
        @media (max-width: 768px) {.cookie-banner {padding: 12px;}.cookie-banner-content {flex-direction: column;gap: 12px;}.cookie-banner-text p {font-size: 11px;}.cookie-banner-actions {width: 100%;}.cookie-btn {flex: 1;}}
    </style>
</head>
<body>

<div class="page-wrapper content-page article-page article-aftercheck">
    <header>
        <a href="../index.html" class="brand" aria-label="jchowlabs home">
            <img src="../static/images/jchowlabs-logo.png" alt="jchowlabs" />
        </a>
        <button class="hamburger" onclick="toggleMenu()" aria-label="Toggle menu">
            <span></span>
            <span></span>
            <span></span>
        </button>
        <nav>
            <ul>
                <li><a href="../index.html">Home</a></li>
                <li><a href="../insights.html">Insights</a></li>
                <li><a href="../research.html">Research</a></li>
                <li><a href="../lab.html" class="active">Lab</a></li>
                <li><a href="#" onclick="openEcosystemModal(event)">Ecosystem</a></li>
                <li><a href="#" onclick="openModal(event)" class="contact-icon" aria-label="Contact">
                    <svg width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round">
                        <rect x="2" y="4" width="20" height="16" rx="2"/>
                        <path d="m22 7-8.97 5.7a1.94 1.94 0 0 1-2.06 0L2 7"/>
                    </svg>
                </a></li>
            </ul>
        </nav>
    </header>
    <main>
        <section class="article-content-section">
            <div class="article-container">
                <div class="article-hero">
                    <img src="../static/images/aftercheck-how-it-works.gif" alt="AfterCheck" class="article-hero-img">
                </div>
                
                <a href="https://www.aftercheck.ai" target="_blank" style="text-decoration: none; display: block;">
                    <div class="try-yourself-cta" style="background: #f0f7ff; border: 1px solid #2563eb; padding: 24px 36px; margin: 35px 0; border-radius: 12px; cursor: pointer; transition: all 0.2s; box-shadow: 0 2px 8px rgba(37, 99, 235, 0.1); display: flex; align-items: center; gap: 16px;">
                        <h3 style="color: #2563eb; margin: 0; font-size: 1.125rem; font-weight: 600; white-space: nowrap;">Try yourself:</h3>
                        <p style="color: #1e40af; margin: 0; line-height: 1.6;">
                            <span class="try-yourself-copy">Get the AfterCheck extension at </span>
                            <span class="try-yourself-url" style="font-weight: 600;">www.aftercheck.ai</span>
                            <span class="try-yourself-copy">.</span>
                        </p>
                    </div>
                </a>
                
                <div class="article-header">
                    <h1 style="font-size: 2rem;">The Story Behind AfterCheck</h1>
                </div>
                
                <div class="article-body">
                    <p>AfterCheck didn't start as a fact-checking product — it started as an LLM rating system.</p>
                    
                    <p>A friend and I had just built a product focused on pricing security vulnerabilities — an app that rates security vulnerabilities in order to derives price estimates for bug bounty programs. Coming off that work, we riffed on a new idea that bubbled up to a rating system for LLM responses across dimensions like security, bias, ethics, safety, and trust.</p>
                    
                    <p>It was the beginning of 2025, and I had just began my year of entrepreneurship. We were both early users of LLMs and encountered the same issues many others were experiencing — hallucinations, factual inaccuracies, biased framing, ethical concerns, and safety-related problems. To be fair, the technology was still in its infancy and evolving rapidly. But at the same time, adoption of products were growing exponentially, which made these issues feel more urgent and worth deeper exploration.</p>
                    
                    <p>Classic me, I built a prototype in about a week — and it was terrible.</p>
                    
                    <p>Not because it didn't work, but because the output was fundamentally unhelpful. The prototype rated each LLM response across bias, ethics, safety, and trust, producing numerical scores for users to interpret. The prototype delivered on the requirements, but in reality, the scores were too abstract for the general user to derive any meaningful value from it.</p>
                    
                    <p>Not all was lost.</p>
                    
                    <p>We had a tangible prototype and an idea that still felt directionally correct — we just needed to refine our understanding of the problem. In classic product management fashion, we started observing how people were using LLMs and focused on nuances where the LLM output triggered some form of skepticism and more importantly, what they naturally did as a result. Patterns emerged, which we further confirmed through broader and more focused surveys. People largely cared about one thing:<strong> factuality.</strong></p>
                    
                    <p>People were not really worried about whether a response was biased or unsafe — they were struggling to tell whether elements within it were correct. At the time, LLM outputs often sounded confident while containing incorrect, outdated, or unsupported claims. As a result, identifying these issues required constant skepticism and a tendency toward excessive, manual, and time-consuming verification.</p>
                    
                    <p>I was already familiar with solutions attempting to address these problems. On the one hand, LLM providers were building in safety, trust, and security mechanisms directly into their platforms — but largely focused on preventing harmful or unsafe outputs at the point where generation happens.</p>
                    
                    <p>On the other hand, a wave of third-party tools were also emerging. These tools were often simple web-apps with interfaces for users to copy and paste LLM responses into. Many focused on detecting whether content was human versus AI-generated, while others specialized in deepfake detection. I was not aware of any products tackling the factuality problem head on, which seemed like a blue ocean opportunity.</p>
                    
                    <p>All of this led us to the following question:</p>
                    
                    <p><strong>Could we build an easy-to-use product that fact-checks LLM responses quickly and reliably — directly inside the tools people already use?</strong></p>
                    
                    <p>The outcome of this question became AfterCheck.</p>
                    
                    <h2>Product Overview</h2>
                    <p>AfterCheck is a Chromium-based browser extension that automatically reveals misinformation in ChatGPT, Claude and Perplexity responses. The extension monitors for conversations between the user and the underlying language model so it can capture both the user's question and the model's response.</p>
                    
                    <p>When a response is captured, the extension sends the question–response pair for fact-checking, then automatically highlights inaccurate claims directly on the original LLM response. Users can hover over a highlighted claim to see additional context — including a correction, supporting evidence, and a confidence score explaining why the claim was flagged.</p>
                    
                    <div class="article-image-placeholder" style="text-align: center; margin-bottom: 30px;">
                        <img src="../static/images/aftercheck-demo.gif" alt="AfterCheck Demo" style="max-width: 100%; height: auto;">
                    </div>
                    
                    <p>How did we ultimately come up with this solution design?</p>
                    
                    <h2>From Rating To Factuality</h2>
                    <p>AfterCheck started with a little bit of wondering.</p>
                    
                    <p>Our initial idea was a rating system that would score individual LLM responses across bias, ethics, safety, and trust. At the time, model rating systems were increasingly common, but most focused on model performance rather than response quality. Meanwhile, like many users, we were experiencing hallucinations, factual inaccuracies and related on a daily basis. We weren't interested in building another leaderboard. We wanted to explore whether these trust-related concerns could be measured and surfaced in a more tangible, response-level way.</p>
                    
                    <p>At the same time, we converged on a browser extension as solution design. Many of the existing tools required the user to copy and paste LLM output into a separate website for analysis, breaking flow and adding friction. But people already interacted with LLMs inside tools like ChatGPT, Claude, and Perplexity. A browser extension allowed us to easily capture the question and response directly, analyze it in context, and surface insights without forcing users to change how they worked.</p>
                    
                    <p>As we began using early prototypes ourselves and sharing them with others, we started to pivot. While users appreciated the broader idea of trust, what they really cared about was factuality. We took a data driven approach, running multiple surveys to confirm our understanding of the problem and how we were thinking about solving it. Below are results from a few of our survey questions:</p>

                    <p><b>Question</b>: How frequently do you notice hallucinations or factual inaccuracies in LLM responses?</p>
                    
                    <div class="article-image-placeholder" style="text-align: center; margin-bottom: 30px;">
                        <img src="../static/images/notice-inaccuracies.png" alt="Survey: Frequency of Noticing Inaccuracies" style="max-width: 65%; height: auto;">
                    </div>
                    
                    <p><b>Question</b>: What features would you find most valuable in a tool designed to address these issues?</p>
                    
                    <div class="article-image-placeholder" style="text-align: center; margin-bottom: 30px;">
                        <img src="../static/images/value-features.png" alt="Survey: Most Valuable Features" style="max-width: 65%; height: auto;">
                    </div>
                    
                    <p><b>Question</b>: If this tool was available as a browser extension, would you try it?</p>
                    
                    <div class="article-image-placeholder" style="text-align: center; margin-bottom: 30px;">
                        <img src="../static/images/third-party-tool.png" alt="Survey: Willingness to Try" style="max-width: 65%; height: auto;">
                    </div>
                    
                    <p>What started as an LLM rating system was refined through prototyping and user feedback into a browser extension that singularly helps users fact-check individual LLM responses for factuality.</p>

                    <h2>Architecture</h2>
                    <p>AfterCheck consists of two main components:</p>
                    <p>(1) a Chromium-based browser extension, and<br>
                    (2) a proprietary fact-checking pipeline.</p>
                    
                    <div class="article-image-placeholder" style="text-align: center; margin-bottom: 30px;">
                        <img src="../static/images/aftercheck-diagram.png" alt="AfterCheck Architecture" style="max-width: 100%; height: auto;">
                    </div>
                    
                    <p>These components work together to verify the factual accuracy of large language model responses in near real time, directly within the user's existing workflow.</p>
                    
                    <p>The browser extension is responsible for everything that happens inside the user's environment. It detects when a user is interacting with ChatGPT, Claude, or Perplexity, captures the relevant question and response, and manages all client-side behavior — from state tracking and response detection to rendering highlights and hover-over tooltips.</p>
                    
                    <p>Once a question–response pair is captured, it is sent to a backend fact-checking pipeline. At a high level, the backend first extracts claims from the response, provides a true / false verdict for each claim in the context of the original question, then returns the results back to the extension.</p>
                    
                    <p>The extension parses the results, then generates a map of the original response so that inaccurate claims can be highlighted. Highlighted claims include hover-over tooltips that provide additional context such as corrections, confidence levels, and evidence — all without requiring the user to leave their conversation.</p>
                    
                    <p>Lets dig a little deeper into how all of this works.</p>
                    
                    <h2>Detecting and Capturing Conversations</h2>
                    <p>The first step in the AfterCheck journey is detecting when a user initiates a conversation with an LLM, knowing when the LLM has finished responding to that question, then precisely capturing the question-response pair for fact checking.</p>
                    
                    <p>At a high level, there are several established techniques for detecting activity in web applications — each with its own tradeoffs. Options range from monitoring DOM mutations, polling for state changes, or relying on event-based hooks tied to user actions or application lifecycles. Any of these could, in theory, be used to detect when a conversation is happening, determine when a response is complete, and help me capture the resulting content. In practice, each approach differs significantly in signal quality and performance overhead.</p>
                    
                    <p>AfterCheck builds on these techniques to reliably interoperate within ChatGPT, Claude, and Perplexity. The extension combines platform-specific signals and heuristics to identify active conversations and determine when responses are complete.</p>
                    
                    <h3>Understanding the DOM</h3>
                    <p>At the user level, ChatGPT, Claude, and Perplexity have simple, conversation-focused interfaces with a single input box for interacting with the underlying LLM. Under the hood, their DOM structures are moderately complex — and differ in important ways that matter for AfterCheck.</p>
                    
                    <p>The first challenge is detection: how do you reliably identify when a real conversation is actually happening with an LLM?</p>
                    
                    <p>A natural assumption is you monitor the DOM and treat any mutation as signal of an in-progress conversation. This approach breaks down quickly because LLM interfaces are surprisingly noisy. Some interesting observations include page re-rendering or content re-generation due to LLMs updating their internal state. Analytics, telemetry, and general background UI components also contribute to noisy DOMs that give the illusion of activity when there is none.</p>
                    
                    <p>Why does this matter? Because reliable detection is what makes inline fact-checking possible in the first place. AfterCheck needs to capture complete question–response pairs for the fact check pipeline to work effectively. The product also operates on a pay-per-fact-check model so pre-mature or partial captures is a requirement. Admittedly, even though strong detection and capture mechanisms are in place, the extension is vulnerable to DOM changes which have happened several times during my evelopment journey.</p>
                    
                    <p>To solve this, the extension implements lightweight, site-specific DOM mutation observers that activate when the user is on a ChatGPT, Claude, or Perplexity site. Rather than responding to every DOM change, these observers look for specific, high-signal patterns — such as prompt submission events, response streaming behavior, and indicators that a conversation is actively progressing. Only when those signals execute in a specific sequence does the extension proceed with capture.</p>
                    
                    <p>The second challenge is capture: how do you precisely capture both the user's question and corresponding LLM response.</p>
                    
                    <p>To break this down, I studied a vast number of response structures produced across LLM platforms and the underlying DOM representations behind them. The goal was to identify reliable patterns that would allow us to extract content consistently while preserving enough structure for downstream processing.</p>
                    
                    <p>Here are the high level patterns:</p>
                    <ul style="margin-left: 20px; padding-left: 20px;">
                        <li>Responses are usually wrapped in specific container elements with predictable class names or attributes</li>
                        <li>Text content is often split across multiple child elements for formatting purposes</li>
                        <li>Code blocks, lists, and tables have distinct DOM structures that need special handling</li>
                        <li>Streaming responses update the DOM incrementally, requiring careful timing to avoid partial captures</li>
                        <li>Some platforms inject non-content elements (like buttons or metadata) that must be filtered out</li>
                    </ul>
                    
                    <p>Capturing complete responses requires more than just extracting text. In additional to preserving the structure, we also needto normalize content for the fact-checking pipeline — parsing out elements like emojis, images, and code blocks — while still preserving the original structure of the response so inaccurate claims can later be highlighted in the correct location.</p>
                    
                    <p>This ultimately led to the development of site-specific selectors and parsing mechanisms for each platform, allowing the extension to reliably detect conversations, capture responses, normalize content for fact-checking, and preserve enough structural context for accurate highlighting.</p>
                
                    <h3>Determining When a Response Is Complete</h3>
                    <p>Once a conversation is detected, another critical challenge was determining when an LLM response is actually complete. While this was relatively straightforward on some platforms, it proved far more difficult on others.</p>
                    
                    <p>If you pay close attention to how LLMs render responses, you'll notice completion indicators — UI elements such as copy, refresh, share, or feedback buttons. These indicators typically appear only after a response has fully finished generating, making them a reliable indicators on when a response is complete.</p>
                    
                    <p>In many cases, these indicators worked well as anchors for completion detection. However, one random day, I encountered pre-mature captures on one of the platforms and could not understand why. When visually observing an in-progress response, I did not see any completion indicators, yet logs told me otherwise.</p>
                    
                    <p>It turned out that on one of the platforms, completion indicators are injected into the DOM alongside the response but remain invisible until generation finishes. From a DOM perspective, the elements were present, but from a UI perspective, they hadn't yet transitioned into view.</p>
                    
                    <p>To address this, the extension combines completion indicators with additional signals, such as visibility checks and timing heuristics, to reliably determine when a response has truly finished. While standard indicators like copy and share buttons are sufficient on some platforms, others required more creative solutions to avoid premature capture.</p>
                    
                    <h3>Preventing Duplicate Captures</h3>
                    <p>Another major challenge was ensuring that the same LLM response is never fact-checked twice.</p>
                    
                    <p>Users scroll through conversations, refresh pages, switch tabs, and revisit prior responses frequently. Without safeguards, the extension could easily re-capture and re-process content that has already been fact-checked — leading not only to redundant backend processing, but also to unnecessary consumption of fact-check credits (AfterCheck monetizes with a pay-per-fact check payment model).</p>
                    
                    <p>To prevent this, the extension implements a deduplication mechanism based on hash comparisons. Before initiating a fact check, the extension computes a hash of the full question–response pair and compares it against hashes of responses already processed in local storage. If a matching hash is found, the capture is skipped as the core design.</p>
                    
                    <p>We experimented with hashing partial content — i.e. hashing only the first or last few sentences of a response — but found this approach unreliable. Many LLM responses share similar openings or conclusions, which led to collisions and false positives. Given the relatively short length of each response, computing hashes on full responses was far more reliable without significant overhead.</p>
                    
                    <p>This deduplication mechanism also enabled reliability across page refreshes and tab switches. Even if the DOM is reloaded or the user navigates away and returns, the extension recognizes responses it has already processed so it can avoid redundant fact checks.</p>
                    
                    <p>One edge case we are actively working on involves long-running or historical conversations. Currently, if a user opens a lengthy conversation that has not been fact-checked before, the extension may begin fact-checking the entire visible conversation. To improve this experience, I'm adding more functionality that detect these longer spans of existing content and give the user a choice on what they want to fact check and what they want to skip.</p>
                    
                    <h3>Tab Switching and Page Refreshes</h3>
                    <p>Modern browsers introduce additional complexity when users switch tabs or refresh pages. Background tabs may throttle JavaScript execution, delay DOM updates, or pause observers altogether.</p>
                    
                    <p>The extension is designed to continue tracking DOM mutations even when the user navigates away mid-response. This ensures that responses are still captured and fact-checked correctly once generation completes — even if it happens in the background.</p>
                    
                    <p>Our existing deduplication logic was also helpful in ensuring page refreshes did not trigger redundant fact checks for content that had already been processed.</p>
                    
                    <h3>User Feedback and Controls</h3>
                    <p>Once a response is detected and queued for fact-checking, the extension implements several visual mechanisms as well controls that inform the user what is happening: </p>
                    
                    <ul style="margin-left: 20px; padding-left: 20px;">
                        <li>A subtle badge appears on queued responses to indicate fact-checking is in progress</li>
                        <li>Once complete, inaccurate claims are highlighted directly in the response</li>
                        <li>Users can disable auto-checking and manually trigger fact checks on demand</li>
                    </ul>
                    
                    <p>These controls ensure the extension feels assistive rather than intrusive, while still operating automatically by default.</p>
                    
                    <h2>The Fact-Checking Pipeline</h2>
                    <p>Once the extension captures a complete question–response pair, it sends that data to the fact-checking pipeline. While the inner workings of this pipeline are proprietary, the high-level flow can be summarized in three steps:</p>
                    
                    <ol style="margin-left: 20px; padding-left: 20px; margin-bottom: 20px;">
                        <li><strong>Claim extraction:</strong> The LLM response is parsed to identify individual factual assertions</li>
                        <li><strong>Verification:</strong> Each claim is evaluated against authoritative sources and context from the original question</li>
                        <li><strong>Scoring and explanation:</strong> Claims are labeled as accurate or inaccurate, with confidence scores and supporting evidence</li>
                    </ol>
                    
                    <p>The resulting data is packaged into a structured format and returned to the browser extension. From there, the extension maps these results back onto the original LLM response, highlighting inaccurate claims and surfacing additional context through hover-over tooltips.</p>
                    
                    <h2>Highlighting Inaccurate Claims</h2>
                    <p>Once the extension receives results from the backend fact-checking pipeline, it moves into one of the most visible — and technically challenging — stages of the system: highlighting inaccurate claims back in the original LLM response.</p>
                    
                    <p>From the user's perspective, the goal is simple. Inaccurate claims should be clearly highlighted, and hovering over a highlighted span should surface additional context — including a correction, a confidence score, and supporting evidence. Achieving this reliably across different platforms and response structures was another engineering challenge.</p>
                    
                    <h3>A Multi-Stage Highlighting Algorithm</h3>
                    <p>Once results are returned from the fact-checking pipeline, the extension must determine where — and how — to highlight inaccurate claims in the original response. This turned into a significant engineering challenge for a number of reasons.</p>
                    
                    <p>At a high level, claims can vary widely in how closely they resemble the original text. In some cases, a claim maps directly back to a contiguous span of text. In others, the relationship is far more indirect — spread across multiple sentences, paraphrased, or implied through context rather than explicitly stated.</p>
                    
                    <p>Because of this variability, the system relies on a multi-stage highlighting algorithm rather than a single matching strategy.</p>
                    
                    <p>The first stage attempts an exact phrase match. If an extracted claim corresponds directly to a verbatim span in the LLM response, the extension can confidently highlight that span. This is the simplest and most precise case, and when it works, it produces clean and intuitive highlights.</p>
                    
                    <p>However, exact matches are often the exception rather than the rule.</p>
                    
                    <p>LLMs frequently paraphrase information, restructure sentences, or distribute claims across multiple parts of a response. A single factual assertion may be expressed using different wording, split across sentences, or implied through surrounding context. In these cases, an exact phrase match fails.</p>
                    
                    <p>When that happens, the algorithm progresses to additional stages that use fuzzy matching and related alignment techniques to infer where the claim most likely originated. These stages reason about similarity rather than identity, allowing the extension to locate the best candidate region of text even when wording differs.</p>
                    
                    <p>The goal of the multi-stage approach is not to find any match, but to find the most semantically appropriate place to apply a highlight — minimizing false positives while ensuring that genuinely problematic claims are surfaced.</p>
                    
                    <p>To make this possible, the algorithm needs a structured way to reason about text boundaries and meaning. This is where the concept of atomic blocks becomes foundational.</p>
                    
                    <h3>Atomic Blocks: Defining Meaningful Highlight Boundaries</h3>
                    <p>Rather than operating on raw character offsets or arbitrary DOM nodes, the extension works with atomic blocks — discrete, meaningful units of content such as sentences, list items, or table cells.</p>
                    
                    <p>Atomic blocks provide a semantic layer between the extracted claims and the raw DOM. They ensure that highlights are applied to coherent units of meaning rather than fragmented or misleading spans of text.</p>
                    
                    <p>For example, a claim like "AfterCheck is a browser extension" should map cleanly to a single sentence or list item — not be split across formatting boundaries or DOM elements. Atomic blocks make it possible to reason about text in a way that aligns with how humans read and interpret content.</p>
                    
                    <p>Once claims are matched against atomic blocks, the extension can apply highlights confidently, even when claims are paraphrased or distributed across complex response structures.</p>
                    
                    <h3>Filtering Non-Fact-Checkable Content</h3>
                    <p>Before claims can be matched and highlighted, the extension must first determine what not to consider.</p>
                    
                    <p>Not all content in an LLM response is suitable for fact-checking. Some elements are inherently non-factual, ambiguous, or outside the scope of what a factuality system can reasonably evaluate. To avoid misleading or confusing highlights, the extension explicitly filters out several categories of content during capture and highlighting.</p>
                    
                    <p>These include:</p>
                    <ul style="margin-left: 20px; padding-left: 20px;">
                        <li>Code blocks and technical snippets that represent implementations rather than assertions</li>
                        <li>Subjective opinions, recommendations, or value judgments</li>
                        <li>Hypothetical scenarios or speculative statements</li>
                    </ul>
                    
                    <p>By excluding these elements early, the system narrows its focus to text-based factual assertions — the type of content where fact-checking is both meaningful and actionable. This filtering also simplifies downstream processing by reducing noise and ambiguity during claim extraction and highlighting.</p>
                    
                    <h3>Accounting for Platform-Specific Structure</h3>
                    <p>Even after content has been normalized into atomic blocks, structural differences across platforms still matter.</p>
                    
                    <p>Each supported LLM interface presents content differently, and responses often blend multiple structures within a single answer. A claim may appear as a list item, a table cell, or several sentences deep inside a dense paragraph. In some cases, the subject of a claim is introduced early, while the factual assertion appears much later and relies on implicit references.</p>
                    
                    <p>The highlighting algorithm must account for these structural nuances when mapping claims back to text. This requires awareness of how atomic blocks relate to one another within the broader response — for example, understanding that multiple sentences belong to the same conceptual paragraph, or that a table row represents a distinct factual unit.</p>
                    
                    <p>By preserving enough structural context from the original DOM, the extension can accurately place highlights in a way that feels intuitive to users, even when responses are complex or deeply nested.</p>
                    
                    <h3>Adapting to Constant UI Changes</h3>
                    <p>One final challenge is that none of these platforms are static.</p>
                    
                    <p>ChatGPT, Claude, and Perplexity all update their interfaces regularly — sometimes in ways that subtly alter DOM structure, class names, or element hierarchies. A selector or heuristic that works perfectly one week may break the next if a platform ships a UI redesign.</p>
                    
                    <p>To address this, AfterCheck implements monitoring and fallback mechanisms that detect when expected DOM patterns change. When possible, the extension adapts gracefully using more general selectors or alternative signals. When adaptation isn't possible, the extension logs detailed diagnostics and alerts our team so we can investigate and ship an update.</p>
                    
                    <p>This ongoing maintenance is essential to keeping AfterCheck reliable across supported platforms. While the core fact-checking logic remains stable, the integration layer must evolve alongside the platforms it supports.</p>
                    
                    <h2>Conclusion</h2>
                    <p>AfterCheck began as a broad exploration of trust in LLM responses and evolved into a focused product that helps users verify factual accuracy in real time. The journey from abstract rating system to inline fact-checking tool taught me valuable lessons about product iteration, user research, and the importance of building solutions that fit seamlessly into existing workflows.</p>
                    
                    <p>Today, AfterCheck is available as a browser extension for Chromium-based browsers, supporting ChatGPT, Claude, and Perplexity. It's designed to be unobtrusive yet effective — working quietly in the background until it finds something worth highlighting.</p>
                    
                    <p>If you're interested in trying AfterCheck or learning more about how it works, visit <a href="https://www.aftercheck.ai" target="_blank">aftercheck.ai</a>.</p>
                </div>
            </div>
        </section>
    </main>
    <footer>
        <span>© jchowlabs, llc</span>
    </footer>
</div>

<!-- Cookie Consent Banner -->
<div class="cookie-banner" id="cookieBanner"><div class="cookie-banner-content"><div class="cookie-banner-text"><p><strong>Privacy Notice:</strong> We use analytics to understand how visitors use site.</p></div><div class="cookie-banner-actions"><button class="cookie-btn cookie-btn-decline" onclick="handleCookieConsent(false)">Decline</button><button class="cookie-btn cookie-btn-accept" onclick="handleCookieConsent(true)">Allow Analytics</button></div></div></div>

<!-- Ecosystem Modal -->
<div class="modal-overlay" id="ecosystemModal">
	<div class="modal" style="max-width: 540px;">
		<h2>Ecosystem</h2>
		<div style="margin-bottom: 24px; line-height: 1.6;">
			<p style="margin-bottom: 16px;">jchowlabs operates within a broad ecosystem of AI and security platforms. We collaborate with technology providers, open-source communities, and industry practitioners to help organizations design, evaluate, and deploy solutions that advance key initiatives and business objectives.</p>
			<p style="margin-bottom: 16px;">As we formalize select partnerships, our advisory approach is informed by hands-on experience across a diverse set of technologies, with recommendations shaped by desired outcomes, organizational context, and long-term operational fit.</p>
			<p style="margin-bottom: 0;">This ecosystem reflects the technologies and platforms we work with today and will naturally expand as new capabilities emerge and client needs evolve.</p>
		</div>
		<button type="button" class="form-submit" onclick="closeEcosystemModal()">Close</button>
	</div>
</div>

<script src="../static/app.js"></script>
<script>
	function checkCookieConsent() {const consent = JSON.parse(localStorage.getItem('cookieConsent') || 'null');if (!consent) {document.getElementById('cookieBanner').classList.add('show');}}
	function handleCookieConsent(accepted) {const consent = {analytics: accepted,timestamp: Date.now(),version: 1};localStorage.setItem('cookieConsent', JSON.stringify(consent));document.getElementById('cookieBanner').classList.remove('show');if (accepted && typeof loadGoogleAnalytics === 'function') {loadGoogleAnalytics();}}
	checkCookieConsent();

	// Ecosystem modal functions
	function openEcosystemModal(e) {
		e.preventDefault();
		document.getElementById('ecosystemModal').classList.add('active');
	}
	
	function closeEcosystemModal() {
		document.getElementById('ecosystemModal').classList.remove('active');
	}
</script>

</body>
</html>
