<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>jchowlabs</title>
    
    <!-- Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-GBHGE9LDVJ"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-GBHGE9LDVJ');
    </script>

    <!-- Google reCAPTCHA -->
    <script src="https://www.google.com/recaptcha/api.js" async defer></script>
    
    <link rel="icon" type="image/png" href="../static/images/favicon.png">
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">
    <link rel="stylesheet" href="../static/styles.css">
    <style>
        .brand {
            font-size: 20px;
            display: inline-flex;
            align-items: center;
        }

        .brand img {
            height: 35px;
            width: auto;
            display: block;
        }
        
        nav a {
            font-size: 14px;
        }
        
        nav a.contact-icon {
            display: inline-flex;
            align-items: center;
            justify-content: center;
            vertical-align: middle;
            margin-left: 8px;
        }
        
        nav a.contact-icon svg {
            color: #888;
            transition: color 0.2s;
        }
        
        nav a.contact-icon:hover svg {
            color: #1a1a1a;
        }
        
        header {
            border-bottom: none;
            position: relative;
            padding-left: 80px;
            padding-right: 80px;
        }
        
        header::after {
            content: '';
            position: absolute;
            bottom: 0;
            left: 48px;
            right: 48px;
            height: 1px;
            background: #f0f0f0;
        }
        
        footer {
            border-top: none;
            position: relative;
        }
        
        footer::before {
            content: '';
            position: absolute;
            top: 0;
            left: 48px;
            right: 48px;
            height: 1px;
            background: #f0f0f0;
        }
        
        /* Active navigation styling */
        nav a.active {
            background: #e0e0e0;
            color: #1a1a1a;
            padding: 8px 16px;
            border-radius: 6px;
        }
        
        nav ul {
            gap: 24px;
        }
        
        .modal h2,
        .modal > p {
            text-align: center;
        }
        
        .modal h2 {
            margin-bottom: 20px;
        }
        
        .form-group {
            margin-bottom: 8px;
        }
        
        .field-error {
            color: #dc2626;
            font-size: 12px;
            margin-top: 4px;
            min-height: 0;
        }
        
        .form-group input.error,
        .form-group textarea.error {
            border-color: #dc2626;
        }
        
        /* Grid layout improvements */
        .content-page {
            height: 100vh;
            display: flex;
            flex-direction: column;
            overflow: hidden;
        }
        
        .content-page main {
            padding: 24px 80px 2px;
            flex: 1;
            display: flex;
            flex-direction: column;
            overflow-y: auto;
            overflow-x: hidden;
        }
        
        .articles-layout {
            flex: 1;
            display: grid;
            grid-template-columns: 1fr 1.3fr;
            gap: 20px;
            align-items: stretch;
            min-height: 0;
        }
        
        .article-featured {
            display: flex;
            flex-direction: column;
            min-height: 0;
            padding: 6px;
            border: 0.5px solid #e5e5e5;
            border-radius: 8px;
        }
        
        .article-featured-img {
            background: #fff !important;
            height: 295px;
            flex-shrink: 0;
        }
        
        .article-featured-img img {
            transform: scale(0.64);
            object-fit: contain;
        }
        
        .article-featured-body {
            margin-top: 14px;
        }
        
        .article-tags {
            display: flex;
            flex-wrap: wrap;
            gap: 8px;
            margin-top: 12px;
        }
        
        .article-tag {
            font-size: 11px;
            color: #666;
            background: #f5f5f5;
            padding: 4px 10px;
            border-radius: 4px;
            border: 1px solid #e5e5e5;
        }
        
        .articles-grid {
            display: grid;
            grid-template-columns: 1fr 1fr;
            grid-template-rows: 1fr 1fr;
            gap: 14px;
            min-height: 0;
        }
        
        .article-compact {
            background: #fff;
            border: 0.5px solid #e5e5e5;
            border-radius: 8px;
            overflow: hidden;
            display: flex;
            flex-direction: column;
            transition: transform 0.2s, box-shadow 0.2s;
            min-height: 0;
            padding: 12px;
            box-shadow: 0 2px 8px rgba(0,0,0,0.04);
            cursor: pointer;
        }
        
        .article-compact:hover {
            transform: translateY(-2px);
            box-shadow: 0 4px 16px rgba(0,0,0,0.08);
        }
        
        .article-compact-img {
            width: 100%;
            height: 100px;
            flex-shrink: 0;
            display: flex;
            align-items: center;
            justify-content: center;
            background: #fff;
        }
        
        .article-compact-img img {
            width: 100%;
            height: 100%;
            object-fit: contain;
            transform: scale(0.95);
        }
        
        .article-compact-content {
            padding: 14px;
            flex: 1;
            display: flex;
            flex-direction: column;
        }
        
        .article-compact-content .meta {
            font-size: 10px;
            color: #888;
            text-transform: uppercase;
            letter-spacing: 0.5px;
            margin-bottom: 6px;
        }
        
        .article-compact-content h3 {
            font-size: 18px;
            font-weight: 600;
            color: #1a1a1a;
            margin-bottom: 5px;
            line-height: 1.3;
        }
        
        .article-compact-content p {
            font-size: 12.5px;
            color: #666;
            line-height: 1.4;
        }
        
        .article-compact-tags {
            display: flex;
            flex-wrap: wrap;
            gap: 6px;
            margin-top: 12px;
        }
        
        .article-compact-tag {
            font-size: 10px;
            color: #666;
            background: #f5f5f5;
            padding: 3px 8px;
            border-radius: 3px;
            border: 1px solid #e5e5e5;
        }
        
        /* Lab page specific styles */
        .content-page main .article-featured-img {
            padding-top: 20px;
        }
        
        .content-page main .article-featured-img img {
            transform: scale(0.38);
        }
        
        .content-page main .article-featured-body {
            margin-top: 24px;
        }
        
        .content-page main .article-compact-content {
            padding: 10px;
        }
        
        .content-page main .article-compact-tags {
            margin-top: 8px;
        }
        
        .content-page main .article-compact:nth-child(3) .article-compact-img img {
            transform: scale(0.6);
        }
        
        /* Article-specific styles */
        .article-content-section {
            padding: 20px 0 60px 0;
        }
        
        .article-hero {
            width: 100%;
            max-width: 100%;
            margin-bottom: 40px;
            overflow: hidden;
            border-radius: 12px;
            border: 1px solid #e5e5e5;
            display: flex;
            align-items: center;
            justify-content: center;
        }
        
        .article-hero-img {
            width: 100%;
            max-width: 850px;
            height: auto;
            display: block;
        }
        
        .article-header {
            margin-bottom: 40px;
        }
        
        .article-header h1 {
            font-family: 'Inter', sans-serif;
            font-size: 2.25rem;
            font-weight: 700;
            color: #1a1a1a;
            line-height: 1.2;
            margin: 0;
        }
        
        .article-body {
            font-family: 'Inter', sans-serif;
            font-size: 1rem;
            line-height: 1.8;
            color: #333;
        }
        
        .article-body p {
            margin-bottom: 24px;
            color: #333;
        }
        
        .article-body h2 {
            font-family: 'Inter', sans-serif;
            font-size: 1.75rem;
            font-weight: 700;
            color: #1a1a1a;
            margin-top: 48px;
            margin-bottom: 24px;
            line-height: 1.3;
        }
        
        .article-body h3 {
            font-family: 'Inter', sans-serif;
            font-size: 1.375rem;
            font-weight: 600;
            color: #1a1a1a;
            margin-top: 36px;
            margin-bottom: 20px;
            line-height: 1.3;
        }
        
        .article-body strong {
            font-weight: 600;
            color: #1a1a1a;
        }
        
        .article-body b {
            font-weight: 600;
            color: #1a1a1a;
        }
        
        .article-body ul,
        .article-body ol {
            margin-bottom: 24px;
            padding-left: 24px;
        }
        
        .article-body li {
            margin-bottom: 12px;
            line-height: 1.7;
        }
        
        .article-body a {
            color: #2563eb;
            text-decoration: none;
            border-bottom: 1px solid #93c5fd;
            transition: border-color 0.2s;
        }
        
        .article-body a:hover {
            border-bottom-color: #2563eb;
        }
        
        .article-image-placeholder {
            margin: 40px 0;
            text-align: center;
        }
        
        .article-image-placeholder img {
            max-width: 100%;
            height: auto;
            border-radius: 8px;
            box-shadow: 0 4px 12px rgba(0, 0, 0, 0.1);
        }
        
        /* Responsive adjustments */
        @media (max-width: 1200px) {
            .content-page main {
                padding: 32px 60px;
            }
            
            .articles-layout {
                grid-template-columns: 1fr;
                gap: 20px;
            }
            
            .articles-grid {
                grid-template-columns: 1fr 1fr;
            }
        }
        
        @media (max-width: 768px) {
            header {
                padding: 16px 24px;
            }
            
            header::after {
                left: 24px;
                right: 24px;
            }
            
            footer {
                padding: 16px 24px;
            }
            
            footer::before {
                left: 24px;
                right: 24px;
            }
            
            /* Show Contact text instead of icon in hamburger menu */
            nav a.contact-icon svg {
                display: none;
            }
            
            nav a.contact-icon::after {
                content: "Contact";
            }
            
            nav a.contact-icon {
                margin-left: 0;
            }
            
            .content-page main {
                padding: 24px 20px;
                min-height: auto;
            }
            
            .article-content-section {
                padding: 30px 0;
            }
            
            .article-hero {
                margin-bottom: 30px;
            }
            
            .article-header h1 {
                font-size: 1.875rem;
            }
            
            .article-body {
                font-size: 1rem;
            }
            
            .article-body h2 {
                font-size: 1.5rem;
                margin-top: 36px;
                margin-bottom: 20px;
            }
            
            .article-body h3 {
                font-size: 1.25rem;
                margin-top: 28px;
                margin-bottom: 16px;
            }
            
            .article-featured-img {
                height: 140px !important;
            }
            
            .article-featured-img img {
                transform: scale(0.8) !important;
            }
            
            .article-featured-body {
                padding: 18px;
            }
            
            .article-featured h3 {
                font-size: 18px;
            }
            
            .article-featured p {
                font-size: 14px;
                line-height: 1.6;
            }
            
            .articles-grid {
                grid-template-columns: 1fr;
                gap: 12px;
            }
            
            .article-compact {
                padding: 16px;
            }
            
            .article-compact-img {
                width: 100%;
                height: 120px;
                display: flex;
                align-items: center;
                justify-content: center;
            }
            
            .article-compact-img img {
                transform: scale(0.85) !important;
            }
        }
    </style>
</head>
<body>

<div class="page-wrapper content-page">
    <header>
        <a href="../index.html" class="brand" aria-label="jchowlabs home">
            <img src="../static/images/jchowlabs-logo.png" alt="jchowlabs" />
        </a>
        <button class="hamburger" onclick="toggleMenu()" aria-label="Toggle menu">
            <span></span>
            <span></span>
            <span></span>
        </button>
        <nav>
            <ul>
                <li><a href="../index.html">Home</a></li>
                <li><a href="../insights.html">Insights</a></li>
                <li><a href="../research.html">Research</a></li>
                <li><a href="../lab.html" class="active">Lab</a></li>
                <li><a href="#" onclick="openEcosystemModal(event)">Ecosystem</a></li>
                <li><a href="#" onclick="openModal(event)" class="contact-icon" aria-label="Contact">
                    <svg width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round">
                        <rect x="2" y="4" width="20" height="16" rx="2"/>
                        <path d="m22 7-8.97 5.7a1.94 1.94 0 0 1-2.06 0L2 7"/>
                    </svg>
                </a></li>
            </ul>
        </nav>
    </header>
    <main>
        <section class="article-content-section" style="width: 100%; display: flex; justify-content: center;">
            <div class="article-container" style="max-width: 890px; width: 100%; padding: 0 20px; box-sizing: border-box;">
                <div class="article-hero">
                    <img src="../static/images/openbounty.gif" alt="Open Bounty" class="article-hero-img">
                </div>
                
                <a href="https://www.openbounty.com" target="_blank" style="text-decoration: none; display: block;">
                    <div style="background: #f0f7ff; border: 1px solid #2563eb; padding: 24px 36px; margin: 35px 0; border-radius: 12px; cursor: pointer; transition: all 0.2s; box-shadow: 0 2px 8px rgba(37, 99, 235, 0.1); display: flex; align-items: center; gap: 16px;">
                        <h3 style="color: #2563eb; margin: 0; font-size: 1.125rem; font-weight: 600; white-space: nowrap;">Try yourself:</h3>
                        <p style="color: #1e40af; margin: 0; line-height: 1.6;">Get an estimate at <span style="font-weight: 600;">www.openbounty.com</span>.</p>
                    </div>
                </a>
                
                <div class="article-header">
                    <h1 style="font-size: 2rem;">Building a Pricing Engine from Scratch</h1>
                </div>
                
                <div class="article-body">
                    <p>Here's an interesting question: <strong>how would you price a security vulnerability?</strong></p>
                    
                    <p>That was the question a friend and fellow classmate posed to me during our graduate studies. I thought about it for a long time. The more I thought about it, the more nuanced and difficult the problem became. Pricing a vulnerability isn't just about severity — it's about context, impact, incentives, uncertainty, and many other micro factors that can significantly influence the final price.</p>
                    
                    <p>When the opportunity came to explore this idea as part of our capstone project, I couldn't resist building something from scratch.</p>
                    
                    <p>The outcome of that work became Open Bounty — a pricing engine that estimates the value of security vulnerabilities using real-world bug bounty data.</p>
                    
                    <h2>Why Pricing Vulnerabilities Is Hard</h2>
                    <p>At its core, Open Bounty explores a fundamental asymmetry in security: the gap between bug hunters who discover vulnerabilities and the organizations running bug bounty programs to incentivize responsible disclosure.</p>
                    
                    <p>If you're unfamiliar with bug bounty programs, they exist to encourage researchers to report vulnerabilities rather than sell or exploit them. In theory, everyone benefits. In practice, pricing vulnerabilities is difficult and sometimes results in undesirable outcomes.</p>
                    
                    <p>From an organization's perspective, payouts must be high enough to motivate disclosure, but not so high that they create misaligned incentives or become unsustainable. From a researcher's perspective, limited transparency makes it hard to know whether a vulnerability is being fairly valued.</p>
                    
                    <p>The challenge is that vulnerability pricing is rarely linear. A vulnerability's value depends on far more than its severity score alone. For example:</p>
                    
                    <ul style="margin-left: 20px; padding-left: 20px;">
                        <li><strong>High severity, low impact:</strong> a serious flaw in a rarely used feature may pose limited real-world risk</li>
                        <li><strong>Low severity, high impact:</strong> a seemingly minor issue in a critical or widely used system can be extremely valuable</li>
                        <li><strong>Context-dependent impact:</strong> where and how a vulnerability can be exploited often matters more than the category it falls into</li>
                        <li><strong>Organization size and resources:</strong> larger, well-funded companies often pay more than smaller organizations for similar issues</li>
                        <li><strong>Program norms:</strong> payouts for similar vulnerabilities can vary widely across different bug bounty programs</li>
                        <li><strong>Perceived risk:</strong> subjective assessments of potential damage can influence final payouts</li>
                    </ul>
                    
                    <p>As a result, two nearly identical vulnerabilities can receive very different rewards depending on the vendor or program involved. From the outside, this can make pricing feel arbitrary, even when decisions are made in good faith.</p>
                    
                    <p>This complexity is what made the problem compelling. Open Bounty wasn't about finding a single "correct" price — it was about understanding the patterns hidden in historical payouts and exploring whether those patterns could be surfaced in a useful, data-driven way.</p>
                    
                    <h2>A Data-Driven Approach</h2>
                    <p>Once we framed the problem, the next question became obvious: could historical bug bounty payouts provide a useful baseline for pricing vulnerabilities?</p>
                    
                    <p>The goal was never to build a perfect or authoritative pricing system. Instead, we wanted to ground estimates in reality — using how vulnerabilities had actually been rewarded in the past — while remaining transparent about where those numbers came from.</p>
                    
                    <p>Two principles guided the approach:</p>
                    
                    <ul style="margin-left: 20px; padding-left: 20px;">
                        <li><strong>Real-world data:</strong> estimates should be informed by actual bug bounty payouts, not abstract scoring systems alone</li>
                        <li><strong>Transparency:</strong> every price estimate should be supported by examples that help users understand why a vulnerability might be valued a certain way</li>
                    </ul>
                    
                    <p>With that framing, the project moved from intuition to implementation. We focused on:</p>
                    
                    <ul style="margin-left: 20px; padding-left: 20px;">
                        <li>gathering publicly available bug bounty payout data</li>
                        <li>cleaning and normalizing that data into a usable dataset</li>
                        <li>training a model to estimate reasonable price ranges</li>
                        <li>and building a simple interface that allowed users to explore those estimates alongside real payout examples</li>
                    </ul>
                    
                    <p>At that point, Open Bounty shifted from a conceptual question into an end-to-end product experiment — one that combined data-driven pricing with explainable outputs.</p>
                    
                    <h2>Building the Dataset: Learning from Real Bug Bounty Payouts</h2>
                    <p>The first concrete step was data.</p>
                    
                    <p>If Open Bounty was going to estimate vulnerability prices in a meaningful way, it had to be grounded in how vulnerabilities had actually been rewarded in the real world. That meant assembling a dataset of real bug bounty payouts — pulling from publicly disclosed reports, write-ups, and program disclosures across a wide range of companies, industries, and vulnerability types.</p>
                    
                    <p>This was also my first real exposure to building a machine-learning–driven product from scratch, and it quickly became clear that the model itself was only a small part of the work.</p>
                    
                    <p>The raw data was messy.</p>
                    
                    <p>Payouts were reported in different formats. Severity labels varied across programs. Some reports focused heavily on technical detail, while others emphasized impact. Many entries lacked important context altogether. Before any modeling could happen, the data needed to be cleaned, normalized, and made consistent enough to be usable.</p>
                    
                    <p>That process involved a series of deliberate tradeoffs:</p>
                    
                    <ul style="margin-left: 20px; padding-left: 20px;">
                        <li>Normalizing severity levels and vulnerability categories across programs</li>
                        <li>Ensuring the dataset was representative across industries, company sizes, and geographies</li>
                        <li>Reconciling payout ranges, bonuses, and edge cases</li>
                        <li>Filtering out incomplete or low-signal entries that would introduce noise</li>
                    </ul>
                    
                    <p>This phase was less about achieving perfect coverage and more about building a dataset that was directionally correct. We weren't trying to model every nuance of every program — we were trying to capture broad, meaningful patterns in how vulnerabilities tend to be valued in practice.</p>
                    
                    <p>Working through this step was one of the most educational parts of the project. It reinforced a core lesson of applied machine learning: the quality of the output is tightly coupled to the quality — and thoughtfulness — of the data that goes in.</p>
                    
                    <p>With a cleaned and representative dataset in place, we could finally move on to the next question: how to turn that data into a pricing model that was both useful and explainable.</p>
                    
                    <h2>Turning Data into a Pricing Model</h2>
                    <p>With a cleaned and representative dataset in place, the next challenge was deciding how to turn that data into a pricing model.</p>
                    
                    <p>We quickly learned that predicting a single "correct" payout was unrealistic. The variability across bug bounty programs — and across organizations themselves — was simply too high. Instead, the goal became estimating reasonable price ranges based on a small number of inputs that a researcher would typically know at the time of reporting a vulnerability.</p>
                    
                    <p>Those inputs were chosen deliberately. Each one reflected signals commonly used in real-world programs and contributed meaningfully to inference:</p>
                    
                    <ul style="margin-left: 20px; padding-left: 20px;">
                        <li>Severity indicators</li>
                        <li>Impact-related characteristics</li>
                        <li>Contextual factors learned from historical payout patterns</li>
                    </ul>
                    
                    <p>The model wasn't designed to replace expert judgment. Rather, it provided a data-informed baseline — a way to answer the question:</p>
                    
                    <p><strong>"Given similar vulnerabilities in the past, what might this be worth?"</strong></p>
                    
                    <p>Just as important as the estimate itself was explainability. From the beginning, we wanted every price range to be supported by real examples of bug bounty payouts that influenced the result. This made the output feel grounded in reality rather than opaque or arbitrary.</p>
                    
                    <h2>Designing a Simple Interface</h2>
                    <p>Once the pricing engine worked, the next challenge was making it usable.</p>
                    
                    <p>The interface needed to strike a careful balance. It had to capture enough information about a vulnerability to produce a meaningful estimate, without overwhelming the user with questions. Every input field was intentional — included only if it meaningfully improved inference.</p>
                    
                    <p>The resulting UI was intentionally minimal. Users entered a small set of details about a vulnerability, and the system returned:</p>
                    
                    <ul style="margin-left: 20px; padding-left: 20px;">
                        <li>A price estimate or range</li>
                        <li>A small set of real bug bounty payouts used as reference points</li>
                    </ul>
                    
                    <p>The interface wasn't meant to be definitive or authoritative. It was designed to be fast, intuitive, and transparent — a way to get a quick, data-backed sanity check rather than a final answer.</p>
                    
                    <p>Building this layer reinforced an important lesson: even the best model is ineffective if users don't understand or trust its outputs. By pairing estimates with concrete examples, Open Bounty made pricing decisions easier to reason about — not just easier to compute.</p>
                    
                    <h2>A Feature We Built — and Chose Not to Ship</h2>
                    <p>As Open Bounty evolved, an interesting secondary opportunity emerged.</p>
                    
                    <p>Because the pricing engine was publicly accessible, we had visibility into aggregate usage patterns — what types of vulnerabilities were being queried, when activity spiked, and which categories appeared most frequently. This naturally led to the question: could this data be useful to organizations themselves?</p>
                    
                    <p>We experimented with building an enterprise-facing dashboard that companies could subscribe to. The idea was to provide high-level insights into whether vulnerabilities related to their products or platforms were being searched for, offering an additional signal around potential exposure or researcher interest.</p>
                    
                    <p>From a technical perspective, it worked reasonably well. The dashboard surfaced anonymized trends and patterns, and early prototypes demonstrated clear potential value for security teams.</p>
                    
                    <p>But over time, we became increasingly uncomfortable with the direction.</p>
                    
                    <p>Open Bounty was designed as an open, neutral pricing system — a tool meant to bring transparency to vulnerability valuation. Introducing a paid enterprise layer created the appearance of bias. Even if the system remained technically sound, monetizing access to search patterns risked undermining trust in the pricing engine itself.</p>
                    
                    <p>That tradeoff wasn't worth it.</p>
                    
                    <p>Ultimately, we decided not to ship the enterprise dashboard. Instead, we kept Open Bounty focused on a single purpose: providing a free, data-backed pricing engine available to anyone on the internet.</p>
                    
                    <p>That decision shaped how I think about product design to this day. Sometimes the right choice isn't about what can be built or monetized — it's about what should be built, especially when trust is central to the product's value.</p>
                    
                    <h2>What the Project Taught Me</h2>
                    <p>Open Bounty ended up being one of the most educational projects I've worked on.</p>
                    
                    <p>It gave me the opportunity to build a machine learning–driven product completely from scratch — from framing the problem, to gathering and cleaning data, to training and refining a model, and finally building an interface that allowed people to interact with it. Seeing the entire system come together, end to end, was incredibly valuable.</p>
                    
                    <p>Beyond the technical aspects, the project highlighted the many nuances involved in building these systems in practice. Data is messy. Tradeoffs are constant. Modeling decisions have downstream effects on usability and trust. And the interface — often treated as an afterthought — plays a critical role in whether a model is actually useful.</p>
                    
                    <p>More broadly, Open Bounty reinforced a lesson that has stayed with me since: pricing, trust, and incentives are deeply intertwined. Numbers don't exist in isolation — they shape behavior. Any system that produces estimates or recommendations needs to be designed with that reality in mind.</p>
                    
                    <h2>Closing Thoughts</h2>
                    <p>After shipping Open Bounty and seeing how it was used in the real world, I found myself circling back to the original question my friend had asked me: "How would you price a security vulnerability?"</p>
                    
                    <p>Now, I know at least one way to do it :)</p>

                    <p>Get your price estimate at <a href="https://www.openbounty.com" target="_blank">www.openbounty.com</a>.</p>
                </div>
            </div>
        </section>
    </main>
    <footer>
        <span>© jchowlabs, llc</span>
    </footer>
</div>

<!-- Ecosystem Modal -->
<div class="modal-overlay" id="ecosystemModal">
	<div class="modal" style="max-width: 540px;">
		<h2>Ecosystem</h2>
		<div style="margin-bottom: 24px; line-height: 1.6;">
			<p style="margin-bottom: 16px;">jchowlabs operates within a broad ecosystem of AI and security platforms. We collaborate with technology providers, open-source communities, and industry practitioners to help organizations design, evaluate, and deploy solutions that advance key initiatives and business objectives.</p>
			<p style="margin-bottom: 16px;">As we formalize select partnerships, our advisory approach is informed by hands-on experience across a diverse set of technologies, with recommendations shaped by desired outcomes, organizational context, and long-term operational fit.</p>
			<p style="margin-bottom: 0;">This ecosystem reflects the technologies and platforms we work with today and will naturally expand as new capabilities emerge and client needs evolve.</p>
		</div>
		<button type="button" class="form-submit" onclick="closeEcosystemModal()">Close</button>
	</div>
</div>

<script src="../static/app.js"></script>
<script>
	// Ecosystem modal functions
	function openEcosystemModal(e) {
		e.preventDefault();
		document.getElementById('ecosystemModal').classList.add('active');
	}
	
	function closeEcosystemModal() {
		document.getElementById('ecosystemModal').classList.remove('active');
	}
</script>

</body>
</html>
                    
                    <p>Not because it didn't work, but because the output was fundamentally unhelpful. The prototype rated each LLM response across bias, ethics, safety, and trust, producing numerical scores for users to interpret. The prototype delivered on the requirements, but in reality, the scores were too abstract for the general user to derive any meaningful value from it.</p>
                    
                    <p>Not all was lost.</p>
                    
                    <p>We had a tangible prototype and an idea that still felt directionally correct — we just needed to refine our understanding of the problem. In classic product management fashion, we started observing how people were using LLMs and focused on nuances where the LLM output triggered some form of skepticism and more importantly, what they naturally did as a result. Patterns emerged, which we further confirmed through broader and more focused surveys. People largely cared about one thing:<strong> factuality.</strong></p>
                    
                    <p>People were not really worried about whether a response was biased or unsafe — they were struggling to tell whether elements within it were correct. At the time, LLM outputs often sounded confident while containing incorrect, outdated, or unsupported claims. As a result, identifying these issues required constant skepticism and a tendency toward excessive, manual, and time-consuming verification.</p>
                    
                    <p>I was already familiar with solutions attempting to address these problems. On the one hand, LLM providers were building in safety, trust, and security mechanisms directly into their platforms — but largely focused on preventing harmful or unsafe outputs at the point where generation happens.</p>
                    
                    <p>On the other hand, a wave of third-party tools were also emerging. These tools were often simple web-apps with interfaces for users to copy and paste LLM responses into. Many focused on detecting whether content was human versus AI-generated, while others specialized in deepfake detection. I was not aware of any products tackling the factuality problem head on, which seemed like a blue ocean opportunity.</p>
                    
                    <p>All of this led us to the following question:</p>
                    
                    <p><strong>Could we build an easy-to-use product that fact-checks LLM responses quickly and reliably — directly inside the tools people already use?</strong></p>
                    
                    <p>The outcome of this question became AfterCheck.</p>
                    
                    <h2>Product Overview</h2>
                    <p>AfterCheck is a Chromium-based browser extension that automatically reveals misinformation in ChatGPT, Claude and Perplexity responses. The extension monitors for conversations between the user and the underlying language model so it can capture both the user's question and the model's response.</p>
                    
                    <p>When a response is captured, the extension sends the question–response pair for fact-checking, then automatically highlights inaccurate claims directly on the original LLM response. Users can hover over a highlighted claim to see additional context — including a correction, supporting evidence, and a confidence score explaining why the claim was flagged.</p>
                    
                    <div class="article-image-placeholder" style="text-align: center; margin-bottom: 30px;">
                        <img src="../static/images/aftercheck-demo.gif" alt="AfterCheck Demo" style="max-width: 100%; height: auto;">
                    </div>
                    
                    <p>How did we ultimately come up with this solution design?</p>
                    
                    <h2>From Rating To Factuality</h2>
                    <p>AfterCheck started with a little bit of wondering.</p>
                    
                    <p>Our initial idea was a rating system that would score individual LLM responses across bias, ethics, safety, and trust. At the time, model rating systems were increasingly common, but most focused on model performance rather than response quality. Meanwhile, like many users, we were experiencing hallucinations, factual inaccuracies and related on a daily basis. We weren't interested in building another leaderboard. We wanted to explore whether these trust-related concerns could be measured and surfaced in a more tangible, response-level way.</p>
                    
                    <p>At the same time, we converged on a browser extension as solution design. Many of the existing tools required the user to copy and paste LLM output into a separate website for analysis, breaking flow and adding friction. But people already interacted with LLMs inside tools like ChatGPT, Claude, and Perplexity. A browser extension allowed us to easily capture the question and response directly, analyze it in context, and surface insights without forcing users to change how they worked.</p>
                    
                    <p>As we began using early prototypes ourselves and sharing them with others, we started to pivot. While users appreciated the broader idea of trust, what they really cared about was factuality. We took a data driven approach, running multiple surveys to confirm our understanding of the problem and how we were thinking about solving it. Below are results from a few of our survey questions:</p>

                    <p><b>Question</b>: How frequently do you notice hallucinations or factual inaccuracies in LLM responses?</p>
                    
                    <div class="article-image-placeholder" style="text-align: center; margin-bottom: 30px;">
                        <img src="../static/images/notice-inaccuracies.png" alt="Survey: Frequency of Noticing Inaccuracies" style="max-width: 65%; height: auto;">
                    </div>
                    
                    <p><b>Question</b>: What features would you find most valuable in a tool designed to address these issues?</p>
                    
                    <div class="article-image-placeholder" style="text-align: center; margin-bottom: 30px;">
                        <img src="../static/images/value-features.png" alt="Survey: Most Valuable Features" style="max-width: 65%; height: auto;">
                    </div>
                    
                    <p><b>Question</b>: If this tool was available as a browser extension, would you try it?</p>
                    
                    <div class="article-image-placeholder" style="text-align: center; margin-bottom: 30px;">
                        <img src="../static/images/third-party-tool.png" alt="Survey: Willingness to Try" style="max-width: 65%; height: auto;">
                    </div>
                    
                    <p>What started as an LLM rating system was refined through prototyping and user feedback into a browser extension that singularly helps users fact-check individual LLM responses for factuality.</p>

                    <h2>Architecture</h2>
                    <p>AfterCheck consists of two main components:</p>
                    <p>(1) a Chromium-based browser extension, and<br>
                    (2) a proprietary fact-checking pipeline.</p>
                    
                    <div class="article-image-placeholder" style="text-align: center; margin-bottom: 30px;">
                        <img src="../static/images/aftercheck-diagram.png" alt="AfterCheck Architecture" style="max-width: 100%; height: auto;">
                    </div>
                    
                    <p>These components work together to verify the factual accuracy of large language model responses in near real time, directly within the user's existing workflow.</p>
                    
                    <p>The browser extension is responsible for everything that happens inside the user's environment. It detects when a user is interacting with ChatGPT, Claude, or Perplexity, captures the relevant question and response, and manages all client-side behavior — from state tracking and response detection to rendering highlights and hover-over tooltips.</p>
                    
                    <p>Once a question–response pair is captured, it is sent to a backend fact-checking pipeline. At a high level, the backend first extracts claims from the response, provides a true / false verdict for each claim in the context of the original question, then returns the results back to the extension.</p>
                    
                    <p>The extension parses the results, then generates a map of the original response so that inaccurate claims can be highlighted. Highlighted claims include hover-over tooltips that provide additional context such as corrections, confidence levels, and evidence — all without requiring the user to leave their conversation.</p>
                    
                    <p>Lets dig a little deeper into how all of this works.</p>
                    
                    <h2>Detecting and Capturing Conversations</h2>
                    <p>The first step in the AfterCheck journey is detecting when a user initiates a conversation with an LLM, knowing when the LLM has finished responding to that question, then precisely capturing the question-response pair for fact checking.</p>
                    
                    <p>At a high level, there are several established techniques for detecting activity in web applications — each with its own tradeoffs. Options range from monitoring DOM mutations, polling for state changes, or relying on event-based hooks tied to user actions or application lifecycles. Any of these could, in theory, be used to detect when a conversation is happening, determine when a response is complete, and help me capture the resulting content. In practice, each approach differs significantly in signal quality and performance overhead.</p>
                    
                    <p>AfterCheck builds on these techniques to reliably interoperate within ChatGPT, Claude, and Perplexity. The extension combines platform-specific signals and heuristics to identify active conversations and determine when responses are complete.</p>
                    
                    <h3>Understanding the DOM</h3>
                    <p>At the user level, ChatGPT, Claude, and Perplexity have simple, conversation-focused interfaces with a single input box for interacting with the underlying LLM. Under the hood, their DOM structures are moderately complex — and differ in important ways that matter for AfterCheck.</p>
                    
                    <p>The first challenge is detection: how do you reliably identify when a real conversation is actually happening with an LLM?</p>
                    
                    <p>A natural assumption is you monitor the DOM and treat any mutation as signal of an in-progress conversation. This approach breaks down quickly because LLM interfaces are surprisingly noisy. Some interesting observations include page re-rendering or content re-generation due to LLMs updating their internal state. Analytics, telemetry, and general background UI components also contribute to noisy DOMs that give the illusion of activity when there is none.</p>
                    
                    <p>Why does this matter? Because reliable detection is what makes inline fact-checking possible in the first place. AfterCheck needs to capture complete question–response pairs for the fact check pipeline to work effectively. The product also operates on a pay-per-fact-check model so pre-mature or partial captures is a requirement. Admittedly, even though strong detection and capture mechanisms are in place, the extension is vulnerable to DOM changes which have happened several times during my evelopment journey.</p>
                    
                    <p>To solve this, the extension implements lightweight, site-specific DOM mutation observers that activate when the user is on a ChatGPT, Claude, or Perplexity site. Rather than responding to every DOM change, these observers look for specific, high-signal patterns — such as prompt submission events, response streaming behavior, and indicators that a conversation is actively progressing. Only when those signals execute in a specific sequence does the extension proceed with capture.</p>
                    
                    <p>The second challenge is capture: how do you precisely capture both the user's question and corresponding LLM response.</p>
                    
                    <p>To break this down, I studied a vast number of response structures produced across LLM platforms and the underlying DOM representations behind them. The goal was to identify reliable patterns that would allow us to extract content consistently while preserving enough structure for downstream processing.</p>
                    
                    <p>Here are the high level patterns:</p>
                    <ul style="margin-left: 20px; padding-left: 20px;">
                        <li>Responses are usually wrapped in specific container elements with predictable class names or attributes</li>
                        <li>Text content is often split across multiple child elements for formatting purposes</li>
                        <li>Code blocks, lists, and tables have distinct DOM structures that need special handling</li>
                        <li>Streaming responses update the DOM incrementally, requiring careful timing to avoid partial captures</li>
                        <li>Some platforms inject non-content elements (like buttons or metadata) that must be filtered out</li>
                    </ul>
                    
                    <p>Capturing complete responses requires more than just extracting text. In additional to preserving the structure, we also needto normalize content for the fact-checking pipeline — parsing out elements like emojis, images, and code blocks — while still preserving the original structure of the response so inaccurate claims can later be highlighted in the correct location.</p>
                    
                    <p>This ultimately led to the development of site-specific selectors and parsing mechanisms for each platform, allowing the extension to reliably detect conversations, capture responses, normalize content for fact-checking, and preserve enough structural context for accurate highlighting.</p>
                
                    <h3>Determining When a Response Is Complete</h3>
                    <p>Once a conversation is detected, another critical challenge was determining when an LLM response is actually complete. While this was relatively straightforward on some platforms, it proved far more difficult on others.</p>
                    
                    <p>If you pay close attention to how LLMs render responses, you'll notice completion indicators — UI elements such as copy, refresh, share, or feedback buttons. These indicators typically appear only after a response has fully finished generating, making them a reliable indicators on when a response is complete.</p>
                    
                    <p>In many cases, these indicators worked well as anchors for completion detection. However, one random day, I encountered pre-mature captures on one of the platforms and could not understand why. When visually observing an in-progress response, I did not see any completion indicators, yet logs told me otherwise.</p>
                    
                    <p>It turned out that on one of the platforms, completion indicators are injected into the DOM alongside the response but remain invisible until generation finishes. From a DOM perspective, the elements were present, but from a UI perspective, they hadn't yet transitioned into view.</p>
                    
                    <p>To address this, the extension combines completion indicators with additional signals, such as visibility checks and timing heuristics, to reliably determine when a response has truly finished. While standard indicators like copy and share buttons are sufficient on some platforms, others required more creative solutions to avoid premature capture.</p>
                    
                    <h3>Preventing Duplicate Captures</h3>
                    <p>Another major challenge was ensuring that the same LLM response is never fact-checked twice.</p>
                    
                    <p>Users scroll through conversations, refresh pages, switch tabs, and revisit prior responses frequently. Without safeguards, the extension could easily re-capture and re-process content that has already been fact-checked — leading not only to redundant backend processing, but also to unnecessary consumption of fact-check credits (AfterCheck monetizes with a pay-per-fact check payment model).</p>
                    
                    <p>To prevent this, the extension implements a deduplication mechanism based on hash comparisons. Before initiating a fact check, the extension computes a hash of the full question–response pair and compares it against hashes of responses already processed in local storage. If a matching hash is found, the capture is skipped as the core design.</p>
                    
                    <p>We experimented with hashing partial content — i.e. hashing only the first or last few sentences of a response — but found this approach unreliable. Many LLM responses share similar openings or conclusions, which led to collisions and false positives. Given the relatively short length of each response, computing hashes on full responses was far more reliable without significant overhead.</p>
                    
                    <p>This deduplication mechanism also enabled reliability across page refreshes and tab switches. Even if the DOM is reloaded or the user navigates away and returns, the extension recognizes responses it has already processed so it can avoid redundant fact checks.</p>
                    
                    <p>One edge case we are actively working on involves long-running or historical conversations. Currently, if a user opens a lengthy conversation that has not been fact-checked before, the extension may begin fact-checking the entire visible conversation. To improve this experience, I'm adding more functionality that detect these longer spans of existing content and give the user a choice on what they want to fact check and what they want to skip.</p>
                    
                    <h3>Tab Switching and Page Refreshes</h3>
                    <p>Modern browsers introduce additional complexity when users switch tabs or refresh pages. Background tabs may throttle JavaScript execution, delay DOM updates, or pause observers altogether.</p>
                    
                    <p>The extension is designed to continue tracking DOM mutations even when the user navigates away mid-response. This ensures that responses are still captured and fact-checked correctly once generation completes — even if it happens in the background.</p>
                    
                    <p>Our existing deduplication logic was also helpful in ensuring page refreshes did not trigger redundant fact checks for content that had already been processed.</p>
                    
                    <h3>User Feedback and Controls</h3>
                    <p>Once a response is detected and queued for fact-checking, the extension implements several visual mechanisms as well controls that inform the user what is happening: </p>
                    
                    <ul style="margin-left: 20px; padding-left: 20px;">
                        <li>A subtle badge appears on queued responses to indicate fact-checking is in progress</li>
                        <li>Once complete, inaccurate claims are highlighted directly in the response</li>
                        <li>Users can disable auto-checking and manually trigger fact checks on demand</li>
                    </ul>
                    
                    <p>These controls ensure the extension feels assistive rather than intrusive, while still operating automatically by default.</p>
                    
                    <h2>The Fact-Checking Pipeline</h2>
                    <p>Once the extension captures a complete question–response pair, it sends that data to the fact-checking pipeline. While the inner workings of this pipeline are proprietary, the high-level flow can be summarized in three steps:</p>
                    
                    <ol style="margin-left: 20px; padding-left: 20px; margin-bottom: 20px;">
                        <li><strong>Claim extraction:</strong> The LLM response is parsed to identify individual factual assertions</li>
                        <li><strong>Verification:</strong> Each claim is evaluated against authoritative sources and context from the original question</li>
                        <li><strong>Scoring and explanation:</strong> Claims are labeled as accurate or inaccurate, with confidence scores and supporting evidence</li>
                    </ol>
                    
                    <p>The resulting data is packaged into a structured format and returned to the browser extension. From there, the extension maps these results back onto the original LLM response, highlighting inaccurate claims and surfacing additional context through hover-over tooltips.</p>
                    
                    <h2>Highlighting Inaccurate Claims</h2>
                    <p>Once the extension receives results from the backend fact-checking pipeline, it moves into one of the most visible — and technically challenging — stages of the system: highlighting inaccurate claims back in the original LLM response.</p>
                    
                    <p>From the user's perspective, the goal is simple. Inaccurate claims should be clearly highlighted, and hovering over a highlighted span should surface additional context — including a correction, a confidence score, and supporting evidence. Achieving this reliably across different platforms and response structures was another engineering challenge.</p>
                    
                    <h3>A Multi-Stage Highlighting Algorithm</h3>
                    <p>Once results are returned from the fact-checking pipeline, the extension must determine where — and how — to highlight inaccurate claims in the original response. This turned into a significant engineering challenge for a number of reasons.</p>
                    
                    <p>At a high level, claims can vary widely in how closely they resemble the original text. In some cases, a claim maps directly back to a contiguous span of text. In others, the relationship is far more indirect — spread across multiple sentences, paraphrased, or implied through context rather than explicitly stated.</p>
                    
                    <p>Because of this variability, the system relies on a multi-stage highlighting algorithm rather than a single matching strategy.</p>
                    
                    <p>The first stage attempts an exact phrase match. If an extracted claim corresponds directly to a verbatim span in the LLM response, the extension can confidently highlight that span. This is the simplest and most precise case, and when it works, it produces clean and intuitive highlights.</p>
                    
                    <p>However, exact matches are often the exception rather than the rule.</p>
                    
                    <p>LLMs frequently paraphrase information, restructure sentences, or distribute claims across multiple parts of a response. A single factual assertion may be expressed using different wording, split across sentences, or implied through surrounding context. In these cases, an exact phrase match fails.</p>
                    
                    <p>When that happens, the algorithm progresses to additional stages that use fuzzy matching and related alignment techniques to infer where the claim most likely originated. These stages reason about similarity rather than identity, allowing the extension to locate the best candidate region of text even when wording differs.</p>
                    
                    <p>The goal of the multi-stage approach is not to find any match, but to find the most semantically appropriate place to apply a highlight — minimizing false positives while ensuring that genuinely problematic claims are surfaced.</p>
                    
                    <p>To make this possible, the algorithm needs a structured way to reason about text boundaries and meaning. This is where the concept of atomic blocks becomes foundational.</p>
                    
                    <h3>Atomic Blocks: Defining Meaningful Highlight Boundaries</h3>
                    <p>Rather than operating on raw character offsets or arbitrary DOM nodes, the extension works with atomic blocks — discrete, meaningful units of content such as sentences, list items, or table cells.</p>
                    
                    <p>Atomic blocks provide a semantic layer between the extracted claims and the raw DOM. They ensure that highlights are applied to coherent units of meaning rather than fragmented or misleading spans of text.</p>
                    
                    <p>For example, a claim like "AfterCheck is a browser extension" should map cleanly to a single sentence or list item — not be split across formatting boundaries or DOM elements. Atomic blocks make it possible to reason about text in a way that aligns with how humans read and interpret content.</p>
                    
                    <p>Once claims are matched against atomic blocks, the extension can apply highlights confidently, even when claims are paraphrased or distributed across complex response structures.</p>
                    
                    <h3>Filtering Non-Fact-Checkable Content</h3>
                    <p>Before claims can be matched and highlighted, the extension must first determine what not to consider.</p>
                    
                    <p>Not all content in an LLM response is suitable for fact-checking. Some elements are inherently non-factual, ambiguous, or outside the scope of what a factuality system can reasonably evaluate. To avoid misleading or confusing highlights, the extension explicitly filters out several categories of content during capture and highlighting.</p>
                    
                    <p>These include:</p>
                    <ul style="margin-left: 20px; padding-left: 20px;">
                        <li>Code blocks and technical snippets that represent implementations rather than assertions</li>
                        <li>Subjective opinions, recommendations, or value judgments</li>
                        <li>Hypothetical scenarios or speculative statements</li>
                    </ul>
                    
                    <p>By excluding these elements early, the system narrows its focus to text-based factual assertions — the type of content where fact-checking is both meaningful and actionable. This filtering also simplifies downstream processing by reducing noise and ambiguity during claim extraction and highlighting.</p>
                    
                    <h3>Accounting for Platform-Specific Structure</h3>
                    <p>Even after content has been normalized into atomic blocks, structural differences across platforms still matter.</p>
                    
                    <p>Each supported LLM interface presents content differently, and responses often blend multiple structures within a single answer. A claim may appear as a list item, a table cell, or several sentences deep inside a dense paragraph. In some cases, the subject of a claim is introduced early, while the factual assertion appears much later and relies on implicit references.</p>
                    
                    <p>The highlighting algorithm must account for these structural nuances when mapping claims back to text. This requires awareness of how atomic blocks relate to one another within the broader response — for example, understanding that multiple sentences belong to the same conceptual paragraph, or that a table row represents a distinct factual unit.</p>
                    
                    <p>By preserving enough structural context from the original DOM, the extension can accurately place highlights in a way that feels intuitive to users, even when responses are complex or deeply nested.</p>
                    
                    <h3>Adapting to Constant UI Changes</h3>
                    <p>One final challenge is that none of these platforms are static.</p>
                    
                    <p>ChatGPT, Claude, and Perplexity all update their interfaces regularly — sometimes in ways that subtly alter DOM structure, class names, or element hierarchies. A selector or heuristic that works perfectly one week may break the next if a platform ships a UI redesign.</p>
                    
                    <p>To address this, AfterCheck implements monitoring and fallback mechanisms that detect when expected DOM patterns change. When possible, the extension adapts gracefully using more general selectors or alternative signals. When adaptation isn't possible, the extension logs detailed diagnostics and alerts our team so we can investigate and ship an update.</p>
                    
                    <p>This ongoing maintenance is essential to keeping AfterCheck reliable across supported platforms. While the core fact-checking logic remains stable, the integration layer must evolve alongside the platforms it supports.</p>
                    
                    <h2>Conclusion</h2>
                    <p>AfterCheck began as a broad exploration of trust in LLM responses and evolved into a focused product that helps users verify factual accuracy in real time. The journey from abstract rating system to inline fact-checking tool taught me valuable lessons about product iteration, user research, and the importance of building solutions that fit seamlessly into existing workflows.</p>
                    
                    <p>Today, AfterCheck is available as a browser extension for Chromium-based browsers, supporting ChatGPT, Claude, and Perplexity. It's designed to be unobtrusive yet effective — working quietly in the background until it finds something worth highlighting.</p>
                    
                    <p>If you're interested in trying AfterCheck or learning more about how it works, visit <a href="https://www.aftercheck.ai" target="_blank">aftercheck.ai</a>.</p>
                </div>
            </div>
        </section>
    </main>
    <footer>
        <span>© jchowlabs, llc</span>
    </footer>
</div>

<script src="../static/app.js"></script>

</body>
</html>
